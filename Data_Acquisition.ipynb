{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook will walk you through acquiring the related TNG-100 data before further analysis can be done. \n",
    "\n",
    "Note that the code in ```Figures.ipynb``` will not run unless the relevant data has been downloaded, processed and stored in compatible formats. So this notebook must be run before the figures from the paper (https://arxiv.org/abs/2105.02234) can be generated.\n",
    "\n",
    "### [Note: Some relevant post-processed data is also available in the repository.]\n",
    "\n",
    "If using this data, you can skip steps 3 anf 5 in this notebook. Scroll to the end of the notebook (look under **Some Shortcuts**) for more information on how to use the data included.\n",
    "\n",
    "## 0. _Get API key_\n",
    "\n",
    "**Downloading TNG data requires an API key from the IllustrisTNG server.** If this your first time working with TNG data, visit https://www.tng-project.org/data/docs/api/ and click on _New User Registration_ to create a user account and request an API key. \n",
    "\n",
    "#### Once in possession of your **API key**, navigate to ```simulation_data.__init__``` and set the variable ```API = \"ThisIsMyAPIKeyForIllustrisTNG\"``` with your API key as a string. \n",
    "\n",
    "You do not need to repeat this step during later iterations, unless your API key changes, or the variable ```API``` is not set permanently.\n",
    "\n",
    "## 1. _Import custom functions_\n",
    "\n",
    "First, import the necessary functions. Make sure to provide the right local path to the custom module ```simulation_data```. This module is required for generating the figures from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_data import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from simulation_data.galaxies import GalaxyPopulation\n",
    "my_galaxy_population = GalaxyPopulation()\n",
    "from simulation_data.galaxies.galaxy import get_galaxy_particle_data, get_stellar_assembly_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. _Download particle data_\n",
    "\n",
    "Now, download targeted particle data for galaxies only within the specified mass cut at the specified redshift. This cell downloads particle data for all $z=2$ galaxies within $10^{10.5} \\leq M_{*}/M_{\\odot} \\leq 10^{12}$. \n",
    "\n",
    "Remember to check the path specified in the function ```get_galaxy_particle_data``` in ```simulation_data.galaxies.galaxy``` and make sure it points to a valid local drive. \n",
    "\n",
    "Within the target drive, create a folder titled ```'redshift_'+str(redshift)+'_data``` before running this cell. \n",
    "\n",
    "Note that the ```get_stellar_assembly_data``` needs a pre-existing stellar assembly file to run. Generating Figures 4 and 5 in the ```Figures``` notebook partially depends on the stellar assembly files for $z=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = 2\n",
    "# this initializes the values in simulation_data.galaxies.galaxy_population\n",
    "ids = my_galaxy_population.select_galaxies(redshift=redshift, mass_min=10.5, mass_max=12)\n",
    "\n",
    "#this gets and saves the particle data for each galaxy in our selection\n",
    "for idx in ids:\n",
    "    get_galaxy_particle_data(id=idx, redshift=redshift, populate_dict=False)\n",
    "    # Download Stellar Assembly Files for the chosen redshift before attempting to get particle assembly data\n",
    "    get_stellar_assembly_data(id=idx, redshift=redshift, populate_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. _Find Main Progenitor and Descendent IDs_\n",
    "\n",
    "We now get the ids for the progenitors and descendents of our population identified at $z=2$. \n",
    "\n",
    "This example finds the main progenitor for each galaxy at $z=3$ and the descendant at $z=1.5$. It saves the arrays of ids at each redshift in the file ```redshift_ids.hdf5```. \n",
    "\n",
    "Note that this step may be time-consuming. The intermediate print statements are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2_ids = ids\n",
    "z3_ids = np.array([-1]*(len(ids)))\n",
    "z1_5_ids = np.array([-1]*(len(ids)))\n",
    "\n",
    "\n",
    "#Finding the progenitors at z=3   \n",
    "count = 0\n",
    "print('z=3', z3_ids)\n",
    "for i, id in enumerate(z2_ids):\n",
    "    if z3_ids[i] == -1:\n",
    "        start_url = \"http://www.tng-project.org/api/TNG100-1/snapshots/33/subhalos/\" + str(id)\n",
    "        sub = get(start_url)  \n",
    "        while sub['prog_sfid'] != -1:\n",
    "            # request the full subhalo details of the progenitor by following the sublink URL\n",
    "            sub = get(sub['related']['sublink_progenitor'])\n",
    "            if sub['snap'] == 25:\n",
    "                z3_ids[i] = sub['id'] \n",
    "    count += 1\n",
    "    print(count)\n",
    "with h5py.File('redshift_ids.hdf5', 'a') as f:\n",
    "    d1 = f.create_dataset('z3_ids', data = z3_ids)\n",
    "    d2 = f.create_dataset('z2_ids', data = z2_ids)\n",
    "    \n",
    "#Finding the descendants at z=1.5\n",
    "count = 0\n",
    "print('z=1.5', z1_5_ids)\n",
    "for i, id in enumerate(z2_ids):\n",
    "    if z1_5_ids[i] == -1:\n",
    "        start_url = \"http://www.tng-project.org/api/TNG100-1/snapshots/33/subhalos/\" + str(id)\n",
    "        sub = get(start_url)   \n",
    "        while sub['desc_sfid'] != -1:\n",
    "            # request the full subhalo details of the progenitor by following the sublink URL\n",
    "            sub = get(sub['related']['sublink_descendant'])\n",
    "            if sub['snap'] == 40:\n",
    "                z1_5_ids[i] = sub['id']         \n",
    "    count += 1\n",
    "    print(count)\n",
    "with h5py.File('redshift_ids.hdf5', 'a') as f:\n",
    "    d3 = f.create_dataset('z1.5_ids', data = z1_5_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. _Download particle data for linked ids at different redshifts_\n",
    "\n",
    "For each new set of ids for progenitors and descendants, now repeat the steps from the second cell to get and save the particle data for each galaxy at the new redshift. \n",
    "\n",
    "Remember to create new folders for each redshift you look at. It is not necessary to add the stellar assembly data for these redshifts to generate the figures in the Letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('redshift_ids.hdf5', 'r') as f:\n",
    "    z1_5_ids = f['z1.5_ids'][:]\n",
    "    z3_ids = f['z3_ids'][:]\n",
    "\n",
    "redshift = 1.5\n",
    "for idx in z1_5_ids:\n",
    "    get_galaxy_particle_data(id=idx, redshift=redshift, populate_dict=False)\n",
    "    \n",
    "redshift = 3\n",
    "for idx in z1_5_ids:\n",
    "    get_galaxy_particle_data(id=idx, redshift=redshift, populate_dict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our section on downloading data on individual halos.\n",
    "\n",
    "The following section uses the downloaded data to calculate halo properties necessary for reproducing the figures in the Letter.\n",
    "\n",
    "## 5. _Calculate halo properties from particle data_\n",
    "\n",
    "To speed up analysis, we now calculate and store some halo properties in a separate hdf5 file named ```'galaxy_population_data_'+str(self.redshift)+'.hdf5'```. \n",
    "\n",
    "Remember to finish downloading individual halo data (running sections 1 and 2, obtaining the relevant halo ids for each chosen redshift) before moving on to the steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = 2\n",
    "#this initializes the values in simulation_data.galaxies.galaxy_population\n",
    "ids = my_galaxy_population.select_galaxies(redshift=redshift, mass_min=10.5, mass_max=12)\n",
    "\n",
    "#calculate halo properties and store calculated data\n",
    "my_galaxy_population.get_galaxy_population_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the directions above will help you download and process IllustrsTNG data on your local machine. \n",
    "\n",
    "### You can move on to the ```Figures.ipynb``` notebook after the data has been stored in compatible formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some shortcuts:\n",
    "\n",
    "Walking down merger trees can be time consuming. Stellar assembly files may be hard to come by/compile from scratch. So here are some shortcuts to make life a little easier. \n",
    "\n",
    "# _Relevant post-processed data included_\n",
    "\n",
    "Post-processed data generated from TNG-100 that has been used to create the figures in this Letter are included in ```galaxy_population_data_2.hdf5``` and ```redshift_ids.hdf5```. \n",
    "\n",
    "Each array stores a halo property for individual halos within our chosen mass-cut at $z=2$, arranged by the order of ids in ```'ids'```. These properties can also be easily recalculated from raw TNG-100 data and stored in the same format by following the steps above.\n",
    "\n",
    "This data is stored in a format compatible with the code in ```Figures.ipynb```. However, individual halo files are not included. Following the steps above will help you download particle data for galaxies directly from the IllustrisTNG public resease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
